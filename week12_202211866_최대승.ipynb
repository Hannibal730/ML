{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 기존 코드의 성능\n",
        "Average cost : 0.5783174822602091\n",
        "\n",
        "Average cost : 0.28515324826481975\n",
        "\n",
        "Average cost : 0.22324668927283225\n",
        "\n",
        "Average cost : 0.19196745904186105\n",
        "\n",
        "Average cost : 0.17168690132189401\n"
      ],
      "metadata": {
        "id": "lykTwEMcGD1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 성능개선을 위한 2가지 방안\n",
        "\n",
        "1. LSTM 레이러를 한 개 더 추가\n",
        "2. 임베딩 사이즈에 맞춰서 0을 패딩하던 것뿐만 아니라, 임베딩 사이즈를 넘어가는 것은 임베딩 사이즈로 잘라내기"
      ],
      "metadata": {
        "id": "P-8q1YgbGK8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 성능개선 이후 코드의 성능\n",
        "\n",
        "Average cost : 0.539866693223579\n",
        "\n",
        "Average cost : 0.22340344816823549\n",
        "\n",
        "Average cost : 0.1681050775171835\n",
        "\n",
        "Average cost : 0.13630058616399765\n",
        "\n",
        "Average cost : 0.11316541318274752"
      ],
      "metadata": {
        "id": "RLijrSUeGTMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 결과\n",
        "성능 개선 시도 이후 로스값이 감소했음을 알 수 있다."
      ],
      "metadata": {
        "id": "KHgLxZpzGys1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwxKv7Np5qMj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d2bad2-036a-49db-b484-b2534b75eaf9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY7_UjO06Emi"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import (DataLoader, TensorDataset)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class SpacingRNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(SpacingRNN, self).__init__()\n",
        "\n",
        "        # 전체 음절 개수\n",
        "        self.eumjeol_vocab_size = config[\"eumjeol_vocab_size\"]\n",
        "\n",
        "        # 음절 임베딩 사이즈\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "\n",
        "        # RNN 히든 사이즈\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "\n",
        "        # 분류할 라벨의 개수\n",
        "        self.number_of_labels = config[\"number_of_labels\"]\n",
        "\n",
        "        # 임베딩층: 랜덤 초기화 후 fine-tuning\n",
        "        self.embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size,\n",
        "                                       embedding_dim=self.embedding_size,\n",
        "                                       padding_idx=0)\n",
        "\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        # 첫 번째 LSTM 레이어\n",
        "        self.bi_lstm_1 = nn.LSTM(input_size=self.embedding_size,\n",
        "                                 hidden_size=self.hidden_size,\n",
        "                                 num_layers=1,\n",
        "                                 batch_first=True,\n",
        "                                 bidirectional=True)\n",
        "\n",
        "        # 두 번째 LSTM 레이어 (추가)\n",
        "        self.bi_lstm_2 = nn.LSTM(input_size=self.hidden_size * 2,\n",
        "                                 hidden_size=self.hidden_size,\n",
        "                                 num_layers=1,\n",
        "                                 batch_first=True,\n",
        "                                 bidirectional=True)\n",
        "\n",
        "        # fully_connected layer\n",
        "        self.linear = nn.Linear(in_features=self.hidden_size * 2,\n",
        "                                out_features=self.number_of_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n",
        "        eumjeol_inputs = self.embedding(inputs)\n",
        "\n",
        "        # 첫 번째 LSTM 레이어\n",
        "        hidden_outputs_1, _ = self.bi_lstm_1(eumjeol_inputs)\n",
        "\n",
        "        # 두 번째 LSTM 레이어 (추가)\n",
        "        hidden_outputs_2, _ = self.bi_lstm_2(hidden_outputs_1)\n",
        "\n",
        "        # Dropout\n",
        "        hidden_outputs = self.dropout(hidden_outputs_2)\n",
        "\n",
        "        # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, number_of_labels)\n",
        "        hypothesis = self.linear(hidden_outputs)\n",
        "\n",
        "        return hypothesis\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unh9In2q6OQK"
      },
      "source": [
        "# 데이터를 읽어 리스트에 저장\n",
        "def read_datas(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "        lines = inFile.readlines()\n",
        "    datas = []\n",
        "    for line in lines:\n",
        "        # 입력 문장을 \\t으로 분리\n",
        "        pieces = line.strip().split(\"\\t\")\n",
        "        # 입력 문자열을 음절 단위로 분리\n",
        "        eumjeol_sequence, label_sequence = pieces[0].split(), pieces[1].split()\n",
        "        datas.append((eumjeol_sequence, label_sequence))\n",
        "    return datas\n",
        "\n",
        "# 데이터를 읽고 각각의 딕셔너리 생성\n",
        "# 임베딩하려면 문장을 벡터화해야 한다.\n",
        "def read_vocab_data(eumjeol_vocab_data_path):\n",
        "    label2idx, idx2label = {\"<PAD>\":0, \"B\":1, \"I\":2}, {0:\"<PAD>\", 1:\"B\", 2:\"I\"}\n",
        "    eumjeol2idx, idx2eumjeol = {}, {}\n",
        "\n",
        "    with open(eumjeol_vocab_data_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "        lines = inFile.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        eumjeol = line.strip()\n",
        "        eumjeol2idx[eumjeol] = len(eumjeol2idx)\n",
        "        idx2eumjeol[eumjeol2idx[eumjeol]] = eumjeol\n",
        "\n",
        "    return eumjeol2idx, idx2eumjeol, label2idx, idx2label\n",
        "\n",
        "def load_dataset(config):\n",
        "    datas = read_datas(config[\"input_data\"])\n",
        "    eumjeol2idx, idx2eumjeol, label2idx, idx2label = read_vocab_data(config[\"eumjeol_vocab\"])\n",
        "\n",
        "    # 음절 데이터, 각 데이터의 실제 길이, 라벨 데이터를 담을 리스트\n",
        "    eumjeol_features, eumjeol_feature_lengths, label_features = [], [], []\n",
        "\n",
        "    for eumjeol_sequence, label_sequence in datas:\n",
        "        eumjeol_feature = [eumjeol2idx[eumjeol] for eumjeol in eumjeol_sequence]\n",
        "        label_feature = [label2idx[label] for label in label_sequence]\n",
        "\n",
        "        # 음절 sequence의 실제 길이\n",
        "        eumjeol_feature_length = len(eumjeol_feature)\n",
        "\n",
        "        # 모든 입력 데이터를 고정된 길이로 맞춰주기 위한 padding 처리\n",
        "        # 우리가 가진 임베딩 사이즈는 이미 결정되어 있음. 하지만 입력데이터 사이즈는 매번 다르니까, 그 사이즈를 임베딩 사이즈로 맞춰주기\n",
        "        # 임베딩 사이즈를 초과할 때는 잘라내는 코드 추가하여 성능개선 시도\n",
        "\n",
        "        # 이곳을 채우세요.\n",
        "        if eumjeol_feature_length > config[\"max_length\"]:\n",
        "          eumjeol_feature = eumjeol_feature[:config[\"max_length\"]]\n",
        "          label_feature = label_feature[:config[\"max_length\"]]\n",
        "        else:\n",
        "          eumjeol_feature += [0] * (config[\"max_length\"] - eumjeol_feature_length)\n",
        "          label_feature += [0] * (config[\"max_length\"] - eumjeol_feature_length)\n",
        "\n",
        "\n",
        "        # 변환한 데이터를 각 리스트에 저장\n",
        "        eumjeol_features.append(eumjeol_feature)\n",
        "        eumjeol_feature_lengths.append(eumjeol_feature_length)\n",
        "        label_features.append(label_feature)\n",
        "\n",
        "    # 변환한 데이터를 Tensor 객체에 담아 반환\n",
        "    eumjeol_features = torch.tensor(eumjeol_features, dtype=torch.long)\n",
        "    eumjeol_feature_lengths = torch.tensor(eumjeol_feature_lengths, dtype=torch.long)\n",
        "    label_features = torch.tensor(label_features, dtype=torch.long)\n",
        "\n",
        "    return eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBfpm6Ts6Reg"
      },
      "source": [
        "def train(config):\n",
        "    # RNN 모델 객체 생성\n",
        "    model = SpacingRNN(config).cuda()\n",
        "\n",
        "    # 데이터 읽기\n",
        "    eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label = load_dataset(config)\n",
        "\n",
        "    # 학습 데이터를 batch 단위로 추출하기 위한 DataLoader 객체 생성\n",
        "    train_features = TensorDataset(eumjeol_features, eumjeol_feature_lengths, label_features)\n",
        "    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    # 크로스엔트로피 비용 함수, padding은 계산하지 않음\n",
        "    # 이곳을 채우세요.\n",
        "    loss_func = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "\n",
        "    # 모델 학습을 위한 optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(config[\"epoch\"]):\n",
        "\n",
        "        model.train()\n",
        "        costs = []\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # 역전파 단계를 실행하기 전에 변화도를 0으로 변경\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "            # 음절 데이터, 각 데이터의 실제 길이, 라벨 데이터\n",
        "            inputs, input_lengths, labels = batch[0], batch[1], batch[2]\n",
        "\n",
        "            # 모델 출력 결과 얻어오기\n",
        "            hypothesis = model(inputs)\n",
        "\n",
        "            # hypothesis : (batch_size, max_length, number_of_labels) -> (batch_size*max_length, number_of_labels)\n",
        "            # labels : (batch_size, max_length) -> (batch_size*max_length, )\n",
        "            # 이곳을 채우세요.\n",
        "            cost = loss_func(hypothesis.reshape(-1, len(label2idx)), labels.flatten())\n",
        "\n",
        "            cost.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # batch 단위 cost 값 저장\n",
        "            costs.append(cost.data.item())\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(output_dir, \"epoch_{0:d}.pt\".format(epoch + 1)))\n",
        "\n",
        "        # epoch 별로 평균 loss 값과 정확도 출력\n",
        "        print(\"Average cost : {}\".format(np.mean(costs)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-A9SurF6UgG"
      },
      "source": [
        "# 모델 출력 라벨 sequence와 정답 라벨 sequence를 기반으로\n",
        "# 모델 출력 문장과 정답 문장 출력\n",
        "def make_sentence(inputs, predicts, labels, idx2eumjeol, idx2label):\n",
        "\n",
        "    predict_sentence, correct_sentence = \"\", \"\"\n",
        "\n",
        "    for index in range(len(inputs)):\n",
        "        eumjeol = idx2eumjeol[inputs[index]]\n",
        "        correct_label = idx2label[labels[index]]\n",
        "        predict_label = idx2label[predicts[index]]\n",
        "\n",
        "        # 시작 음절인 경우 공백을 추가해줄 필요가 없음\n",
        "        if (index == 0):\n",
        "            predict_sentence += eumjeol\n",
        "            correct_sentence += eumjeol\n",
        "            continue\n",
        "\n",
        "        # \"B\" 태그인 경우 어절의 시작 음절이므로 앞에 공백을 추가\n",
        "        if (predict_label == \"B\"):\n",
        "            predict_sentence += \" \"\n",
        "        predict_sentence += eumjeol\n",
        "\n",
        "        # \"B\" 태그인 경우 어절의 시작 음절이므로 앞에 공백을 추가\n",
        "        if (correct_label == \"B\"):\n",
        "            correct_sentence += \" \"\n",
        "        correct_sentence += eumjeol\n",
        "\n",
        "    return predict_sentence, correct_sentence\n",
        "\n",
        "# 텐서를 리스트로 변환하는 함수\n",
        "def tensor2list(input_tensor):\n",
        "    return input_tensor.cpu().detach().numpy().tolist()\n",
        "\n",
        "def test(config):\n",
        "    # 데이터 읽기\n",
        "    eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label = load_dataset(config)\n",
        "\n",
        "    # 평가 데이터를 batch 단위로 추출하기 위한 DataLoader 객체 생성\n",
        "    test_features = TensorDataset(eumjeol_features, eumjeol_feature_lengths, label_features)\n",
        "    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=1)\n",
        "\n",
        "    # RNN 모델 객체 생성\n",
        "    model = SpacingRNN(config).cuda()\n",
        "    # 사전학습한 모델 파일로부터 가중치 불러옴\n",
        "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"model_name\"])))\n",
        "\n",
        "    # 모델의 출력 결과와 실제 정답값을 담을 리스트\n",
        "    total_hypothesis, total_labels = [], []\n",
        "\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "\n",
        "        model.eval()\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "        # 음절 데이터, 각 데이터의 실제 길이, 라벨 데이터\n",
        "        inputs, input_lengths, labels = batch[0], batch[1], batch[2]\n",
        "\n",
        "        # 모델 평가\n",
        "        hypothesis = model(inputs)\n",
        "\n",
        "        # (batch_size, max_length, number_of_labels) -> (batch_size, max_length)\n",
        "        hypothesis = torch.argmax(hypothesis, dim=-1)\n",
        "\n",
        "        # batch_size가 1이기 때문\n",
        "        input_length = tensor2list(input_lengths[0])\n",
        "        input = tensor2list(inputs[0])[:input_length]\n",
        "        label = tensor2list(labels[0])[:input_length]\n",
        "        hypothesis = tensor2list(hypothesis[0])[:input_length]\n",
        "\n",
        "        # 출력 결과와 정답을 리스트에 저장\n",
        "        total_hypothesis += hypothesis\n",
        "        total_labels += label\n",
        "\n",
        "        if (step < 10):\n",
        "            # 정답과 모델 출력 비교\n",
        "            predict_sentence, correct_sentence = make_sentence(input, hypothesis, label, idx2eumjeol, idx2label)\n",
        "            print(\"정답 : \" + correct_sentence)\n",
        "            print(\"출력 : \" + predict_sentence)\n",
        "            print()\n",
        "\n",
        "    # 정확도 출력\n",
        "    print(\"Accuracy : {}\".format(accuracy_score(total_labels, total_hypothesis)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj-JT2466U9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff5ba83-ae33-49f5-9917-6f8459ba381e"
      },
      "source": [
        "if(__name__==\"__main__\"):\n",
        "    root_dir = \"/content/drive/MyDrive/ML/rnn\"\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"model_name\":\"epoch_{0:d}.pt\".format(5),\n",
        "              \"input_data\":os.path.join(root_dir, \"train.txt\"),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"eumjeol_vocab\": os.path.join(root_dir, \"eumjeol_vocab.txt\"),\n",
        "              \"label_vocab\": os.path.join(root_dir, \"label_vocab.txt\"),\n",
        "              \"eumjeol_vocab_size\": 2458,\n",
        "              \"embedding_size\": 100,\n",
        "              \"hidden_size\": 100,\n",
        "              \"max_length\": 920,\n",
        "              \"number_of_labels\": 3,\n",
        "              \"epoch\":5,\n",
        "              \"batch_size\":64,\n",
        "              \"dropout\":0.3\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cost : 0.539866693223579\n",
            "Average cost : 0.22340344816823549\n",
            "Average cost : 0.1681050775171835\n",
            "Average cost : 0.13630058616399765\n",
            "Average cost : 0.11316541318274752\n"
          ]
        }
      ]
    }
  ]
}